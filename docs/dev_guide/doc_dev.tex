\documentclass[12pt]{report}
\usepackage{graphicx}
\usepackage{float}
\usepackage{hyperref}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\usepackage{listings}

\begin{document}
\begin{titlepage}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for the horizontal lines, change thickness here

\center 

\HRule \\[0.4cm]
{ \huge \bfseries Developer Documentation: \\Representation and relative positioning from visual information}\\[0.4cm]
\HRule \\[1.5cm]

\begin{minipage}{0.4\textwidth}
	\begin{flushleft} \large
		\emph{Realized by:}\\
		\textsc{Asma BRAZI}
	\end{flushleft}
\end{minipage}
~
\begin{minipage}{0.4\textwidth}
	\begin{flushright} \large
		\emph{Proposed by:} \\
		\textsc{CÃ©dric HERPSON}\\
	\end{flushright}
\end{minipage}\\[4cm]


{\large Laboratory of Computer Sciences, Paris 6 \\ Sorbonne University - Faculty of Science and Engineering}\\[3cm] 
{\large June-July 2019 }\\[3cm] 
\includegraphics[width=0.6\textwidth]{logo.png}\\[1cm] 
\vfill % Fill the rest of the page with whitespace

\end{titlepage}
\tableofcontents

\chapter{Introduction}
\paragraph{}
This document represents a guide for developers. The project has been built so that it can be extremely adaptable. So, it defines precisely each function developed. Also, it describes how some features can be improved.
\paragraph{}
We precise that this document covers only the main developed and updated features. For more details about the other features developed previously, please visit the project at \url{https://gitlab.com/asmabrz/pandroide}

\paragraph{}
The development of the project is divided into small applications. 


\chapter{Code organization}
\paragraph{}
The project is composed of two independent parts communicating with each others. The first part runs on a computer and it concerns the exploration strategy. The second one runs on a Raspberry-Pi and it concerns the image processing and the robot's movements. In what follows, the two parts are developed.
\section{Computer side}
\paragraph{}
When the robot collects visual information, It sends them to the computer. According to those visual information, the next move is decided from the exploration strategy, and sent to the robot. The code on the computer covering the exploration strategy and the communication between the robot and the computer is written in Java. We wrote the code on Java because the previous works were written in \textbf{Java} ( \url{https://www.java.com} ).
\paragraph{}
The visual information received by the computer may be walls or obstacles. This knowledge is represented in real time on a 3D map using \textbf{jMonkey Engine} at \url{http://jmonkeyengine.org}. The map is updated every time the robot explores a new element of the environment.
\paragraph{}
The most important functions developed are:
\begin{itemize}
    \item  \textbf{void simpleInitApp():} Initialize the 3D render. 
    \item \textbf{void simpleUpdate(float tpf):} Update the 3D render periodically.
    \item \textbf{sendToPC(String objToReceive,String dir):} Receive on the computer an object sent by the robot.
    \item \textbf{void initPi():} Send the database of objects and scripts for image processing and robot movements to the robot.
    \item \textbf{void executeCommandRPi(String command,boolean getResult):} Order to the robot to execute the command mentioned in arguments. the parameter \textbf{getResult} specifies whether this command returns a result or not.
    \item \textbf{void rotate(double angle):} Order to the robot to rotate with a specific angle. If the angle is negative, the robot rotates left. 
    \item \textbf{float move(double distanceRob):} Order to the robot to travel the specified distance. If the distance is negative, the robot moves back.
    \item \textbf{float updatePosition() :} After a movement, the robot's position changes. this function updates the robot's position accoding to the distance traveled.
    \item \textbf{List$<$ArrayList$<$Float$>>$  captureData():} Order the robot to collect visual information and to send them to the computer.
    \item \textbf{int explore(int target):} Order to the robot to explore the environment and to search for a target.
    \item \textbf{float pixToCmY(float x):} Convert to centimeters a distance measured in pixel between the robot and a specified object.
    \item \textbf{float pixToCmX(float x1,float x2):} Convert to centimeters an object width according to its width measured in pixels and the real distance from the robot in centimeters.
\end{itemize}
\section{Raspberry-Pi side}
\paragraph{}
From the start of the project, this part has been mainly written in Python, and we kept that choice. The principal modules used in the development are:
\begin{itemize}
    \item OpenCV for image processing (Hough lines, LSD, Canny...etc).
    \item Picamera to manipulate a Raspberry-Pi camera (Take a picture).
    \item Aseba to manipulate a Thymio-II robot (Rotation, movement, feedback from sensors...etc).
\end{itemize}
\paragraph{}
The most important functions developed are:
\begin{itemize}
    \item \textbf{takePicture(r1=600,r2=400):} The robot takes a picture with the Raspberry-Pi. The resolution me bay specified but not required.
    \item  \textbf{HoughLinesP(im='v.jpg',maxLG=350):} Returns the longest vertical line of an image. The maximum length can be specified with the parameter \textbf{maxLG}.
    \item \textbf{recognition(img,nbRefs=1):} Verify if any element of the database of objects figures on the image specified in parameters.
    \item \textbf{newLineMerging(l1,l2):} Merge the two lines \textbf{l1} and \textbf{l2} into one line.
    \item \textbf{mergeLines(lines,threshold=30):} Given a set of lines, the nearest lines are merged according to a fixed threshold.
    \item \textbf{LSDDetection(im):} Apply LSD algorithm on a picture to detect vertical, horizontal and diagonal lines.
    \item \textbf{moveForward(distCM):} Order the robot to move forward. If an obstacle is detected and the robot travels less than the distance requested, the real distance traveled is returned.
    
\end{itemize}
\end{document}
