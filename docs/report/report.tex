\documentclass[12pt]{report}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{float}
\usepackage{pgfgantt}
\usepackage{hyperref}
\usepackage{algorithm2e}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}

\usepackage[toc]{glossaries}
\usepackage{appendix}

\usepackage{mathptmx}

\usepackage{cite} %for bibliography

\makeglossaries


% Comments
\usepackage{color}
\usepackage[normalem]{ulem} %pour le format barré
\newcommand{\hcp}[1]{\textcolor{blue}{[#1]}}
\newcommand{\hcr}[2]{\textcolor{red}{\sout{[#1]} - \textcolor{blue}{ [#2]}}}
\newcommand{\hc}[1]{\textcolor{red}{[#1]}}
\newcommand{\hcc}[1]{\textcolor{green}{Pour info - [#1]}}


%for acknowledgements
\newenvironment{acknowledgements}[1]
{\renewcommand{\abstractname}{Acknowledgements}
\begin{abstract}}
{\end{abstract}}



\begin{document}

	\begin{titlepage}
		
		\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for the horizontal lines, change thickness here
		
		\center 
		\HRule \\[0.4cm]
		{ \huge \bfseries Report \\Representation and relative positioning from visual information}\\[0.4cm]
		\HRule \\[1.5cm]
		
		\begin{minipage}{0.4\textwidth}
			\begin{flushleft} \large
				\emph{Submitted by:}\\
				\textsc{Asma BRAZI}
			\end{flushleft}
		\end{minipage}
		~
		\begin{minipage}{0.4\textwidth}
			\begin{flushright} \large
				\emph{Supervised by:} \\
				\textsc{Cédric HERPSON}\\
			\end{flushright}
		\end{minipage}\\[4cm]
		
		
		{\large \hcr{Laboratory of Computer Sciences, Paris 6}{LIP6 Computer Science laboratory} \\ Sorbonne 
University - Faculty of Sciences and Engineering}\\[3cm] 
		{\large June - July 2019 }\\[3cm] 
		\includegraphics[width=0.6\textwidth]{res/logo.png}\\[1cm] 
		\vfill % Fill the rest of the page with whitespace
		
	\end{titlepage}
	

	%\chapter{Abstract}
	\begin{abstract}
	 
	\paragraph{}
	We carried out our internship at the \hcr{Laboratory of Computer Sciences, Paris 6 (LIP6)}{LIP6, the computer 
science laboratory of Sorbonne University, Paris, France} under the supervision of Dr. Cédric Herpson, from June 3rd, 
2019 until July 26th, 2019. The internship was mainly considered as a continuation of works that we carried out during 
a university project (PANDROIDE) in our first \hc{year of} Master's degree. 
	
	\paragraph{}
	Our previous works presented a naive approach of object recognition and exploration strategy that allowed an autonomous robot with a camera to roughly build its environment. The environment is closed and composed of walls where the robot was able to locate itself. In addition, we could ask the robot to search for a specific object, and the robot was able to accomplish that task. Nevertheless, the conducted works assumed some hypotheses which did not make the exploration strategy generic.
		
	\paragraph{}
	During this internship, we focused the most on improving the exploration strategy to no longer depend on certain 
hypotheses. For example, in our previous works, we had some specific objects placed on corners to indicate to the robot 
that there were a corner at this spot. This hypothesis allowed the robot not to perform image processing to detect 
corners. 
	
	\paragraph{}
	Our new strategy does not \hcr{depend anymore of these objects}{depends on these objects anymore}. \hcr{But, 
i}{I}t exploits some visual information as contours. Also, the information provided by the robot's \hc{IR} sensors is 
taken into account. We \hc{thus} developed \hcr{so}{} a strategy where no \hc{prior} knowledge of the environment is 
necessary, and no hypotheses helping the robot building its environment are considered.
	
	
	\end{abstract}

	
	%\chapter*{Acknowledgments}
	\begin{acknowledgements}
	\centering
		
	
	I would like to deeply thank my supervisor, Mr. Cédric Herpson, for his guidance and help, his presence and his trust in me to lead the initiative in my work. 
	
	I can say that I have been truly lucky to have Mr. Cédric Herpson as a supervisor who cared so much about my work, although he had a lot of work ahead of him. 
	
	\end{acknowledgements}
		
	\tableofcontents
	\thispagestyle{empty}
	
	\pagenumbering{arabic}
	
	\setcounter{page}{0}
	
	\chapter*{Introduction}
	\addcontentsline{toc}{chapter}{Introduction}
	\paragraph{}
	When an autonomous robot navigates in a structured or unstructured environment, it should be able to build a map 
representing this environment, to localize itself in it, and to define a safe path to move from a place to another 
one.\hc{These tasks are traditionally called Simultaneous Localisation And Mapping (SLAM)}
	
	\paragraph{}
	\hcr{There are a varied applications of the robot 
navigation. Such as}{Applications to autonomous robot navigation are numerous :} surveillance, cleaning, transportation 
tasks...etc. \hcp{Paired with the progresses in term of computational embedded capabilities,} the number of 
contributions of researchers in this field keeps going up \cite{bonin-font_visual_2008}.
	
	\paragraph{}
	Our project aims to enable \hcr{the}{a} Thymio robot\footnote{An educational programmable robot} with a 
\hc{monocular} camera to roughly build its environment. \hc{We assume that } the environment is closed and it is 
composed of walls where the autonomous robot is able to locate itself. \hc{Note that even if we focus on a specific 
platform for the experimentations, our proposal intends to be generic.}
	
	\paragraph{}
	Unlike our previous work where we had some landmarks helping the autonomous robot in the detection of corners, 
we develop throughout th\hcr{e}{is} internship a more generic strategy. The autonomous robot \hcr{is based only on its 
sensors and camera to build }{only use its sensors (IR and camera) to build its own representation of }the environment.
	
	So with this strategy, the robot can manage to build its entire environment autonomously.
	
	
	\section*{Outline of this report}
	\addcontentsline{toc}{section}{Outline of this report}
	\paragraph{}
	\hcr{The remaining chapters are}{This document is} organized as follows:
	\begin{itemize}
		\item Chapter \ref{chap:review}: \hcr{Summarize the state of art}{We realize a brief overview of the 
state of the art related to SLAM}.
		\item Chapter \ref{chap:realization}: \hcr{Explanation of the solution}{We detail the proposed 
approach}.
		\item Chapter \ref{chap:testing}: \hcr{Discussion and test of the solution}{We present our experiments 
and discuss the associated results}.
		%\item Chapter \ref{chap:conlusion} Conclusion and suggestion of future works.
	\end{itemize}
	\hc{Finally we conclude this work and highlight its current limitations before suggesting potential solutions 
and possible future works}
	
	
    \chapter{Reviews of Existing Work}
    \label{chap:review}
	
	\hc{In this chapter, we first propose a brief overview of the literature before presenting the work 
done by our predecessors}.

\section{Brief overview of the litterature}

\hc{Insert here Pandro biblio section}


\section{State of the project}

	The main purpose of our works is to improve the functionalities developed previously. These functionalities 
allowed one autonomous robot to move in a rectangular environment and to explore it.\\


\hc{Insérer un schéma et un petit paragraphe pour présenter les différents composants, thymio+pi+pc et indiquer ou se 
situent les différents traitements; Récupérer de Pandro}


	The visual information brought by the exploration is used to build a virtual 3D representation of 
\hcr{this}{the} environment, and to recognize a target object \hc{that should be found on one of the walls}. The target 
object is \hcr{an object from the }{assumed to be available within the agent } knowledge database. It is specified by 
the user at the launching of the application.
In short, the previous strategy can be summarized as follows:
\hc{Passage au style direct pour plus de clarté}
	\begin{enumerate}
		\item The autonomous robot takes a picture and analyses it.
		\item The analysis generates new knowledge. The robot populates its 3D representation accordingly. 
		\item The robot is then in one of the following situations:
		\begin{itemize}
			\item The target is detected and the exploration ends.
			\item The four walls are explored and the target was not detected. So, the exploration ends.
			\item A corner is detected. As a result, the autonomous robot goes on the next wall.
			\item Beside the wall distance and size, nothing is acquired, the autonomous is ready to move 
to the next part of the same wall.
		\end{itemize}
		\item Execute next move until the exploration ends.
	\end{enumerate}

	
	
	\hcr{However}{As it can be seen}, we assumed some hypotheses which facilitated the exploration process. For 
example, we placed some landmarks in each corner of the environment and every time the robot met 
these objects, it concluded that this was a wall end. So the robot relied on these objects to decide whether 
or not it finished exploring the current wall. Otherwise seen, as long as the autonomous robot did not meet a landmark, 
then it continued moving forward assuming it is the same wall.\\
	
	\hcr{Also, a}{A}nother hypothesis is when the \hcr{autonomous}{} robot did not detect anything, and it assumed 
that it was the continuation of the current wall. Such a hypothesis may be hard to omit if we want to adjust the 
strategy to be usable in a more complex environment. A complex environment may be a room with corridors. In that case, a 
corridor may be ignored, because the \hcr{autonomous}{} robot only took into account the presence of objects for the 
decision and not the structure of the environment.\\
	
	To no longer depend on these hypotheses, \hcr{our works 
present another manner to build the environment}{we decided to change the way we build the environment}. Firstly, we do 
not anymore assume having a rectangular environment \hcr{, but we build}{and propose}  a strategy which may work 
independently of the environment structure.\\
\hcr{Than, the autonomous robot does}{Secondly, we will} not suppose the 
presence of a wall by detecting one of its corners. Instead, \hcr{it extracts }{we will extract} the contours of the 
environment. \hcr{with whom the autonomous robot }{From there, the agent will have to decide} if it is facing a 
close/far wall, if there is a wall behind them...etc. \hcr{	By this way, the autonomous robot gathers the }{We will 
also use images } analysis's results to estimate the dimensions of the walls.\hc{We will present these steps in detail 
in the next chapter}.
	
    \chapter{Realization}
    \label{chap:realization}
    
    We suppose that the autonomous robot is situated in a closed environment, and it has been informed about the target 
\hcr{object to search}{to be looking for}. 


	\section{Exploration strategy}
	 \subsection{Image processing}
	 \paragraph{}
	 The first task \hcr{of the autonomous}{for the} robot is to roughly represent the environment using visual 
information. \hc{For that we rely on segment recognition\hc{Donner une ref présentant les algos associés}, a 
lightweight image process.} For walls detection, we distinguish three types of segments on the picture taken by the 
autonomous robot:
	 \begin{itemize}
	 	\item Vertical segment which makes almost 90 degrees with the X-axis.
	 	\item Horizontal segment which makes almost 0 degree with the  X-axis.
	 	\item Diagonal segment which makes an acute angle with the X-axis.
	 \end{itemize} 
	 
	 In the sections to come, we present the different strategies we have developed and the one we have particularly kept.
	 
	 \subsubsection{Corners-based detection}
	 
	 We begun with a strategy based on corners detection. \hcr{where w}{W}e kept the same exploration strategy as 
in our previous works \hcr{But, we replaced only }{and replaced} the part on corners detection. \hcr{So }{Thus,} 
instead of detecting a specific objects substituting the corners, we tried two methods. 
\begin{itemize}
 \item A hand-made approach based on \textit{segments intersection}
 \item A method for \textit{corners detection} available in OpenCV\hc{ref}.
\end{itemize}
%We developed  and we used  
These two methods \hcr{were used under a certain constraints, }{require to fix a threshold} to decide whether a corner 
as been detected or not. Defining \hcr{these constraints}{this separating limit} is not that easy. For example, it is 
getting arduous if we have a textured floor or walls. So the method considers all the intersection points situated on 
the walls and on the floor as corners.\\
	
	 Also, if the floor and the walls have almost the same color, the segment separating the wall from the floor may not be detected.
	 As a result, these methods detect a lot of features which are not all correct corners.\\	 
	 On the other hand, if we assume that corners are in most cases situated at the bottom of the image, this 
assumption becomes incorrect when the autonomous robot is far from the corner. In this latter case, the corner is 
situated on the top of the image. The figure \ref{fig:corner} below \hc{illustrate this situation}.
	 	\begin{figure}[h]
	 	\begin{center}
	 		\includegraphics[scale=0.6]{res/start1_c1.png}
	 		\caption{An ignored corner because of the corners placement constraint}
	 		\label{fig:corner}
	 	\end{center}
	 \end{figure}
	% \paragraph{}
	As a result, this corner will be ignored, because it is not situated on the bottom of the image.
	 
	% \paragraph{}
	\hc{Pas clair cette phrase dessous}
	 These particular pictures taken by the \hcr{autonomous}{} robot make the strategy not really captivating, 
because it is sensitive to the autonomous robot's position.
	 
	 \subsubsection{Non corners-based detection}
	
	 \hcr{After encountering some difficulties to detect the}{Facing the difficulty to detect the} right corners, 
we decided to ditch this strategy \hc{ because it was impractical}{and to focus on the segment orientation}. 
	 
	 The next idea is \hc{thus} to consider the segments on the 3 axis X,Y and Z. The vertical (resp. horizontal) 
segments are used to estimate the walls height (resp. width). \hcr{However the d}{D}iagonal segments indicate that 
there is a depth. Knowing that a certain depth exists on the image, guarantees to the autonomous robot that there is 
still space between them and the wall in front. So, it can always move forward.
	  
	  We have used the \textit{Line Segment Detector} algorithm (LSD) to detect straight contours on the 
image \cite{grompone_von_gioi_lsd:_2012}. At each pixel of the image, LSD estimates the angle of the gradient. After 
that, the pixels sharing nearly the same angle of gradient are merged into regions. The connected regions are called 
\textit{line support regions}. Then, the algorithm keeps some of these \textit{line support regions} to be a 
\textit{line segment}.
	  
	 
	  The picture below shows an image on which the LSD was applied and as a result, the \textit{line support regions} that were detected.
	  	\begin{figure}[H]
	  	\begin{center}
	  		\includegraphics[scale=0.6]{res/lsr.png}
	  		\caption{Line Support Regions. Adapted from "Rafael Grompone von Gioi, Jérémie Jakubowicz, Jean-Michel Morel, and Gregory Randall, LSD: a Line Segment Detector, Image Processing On Line, 2 (2012)" }
	  	\end{center}
	  \end{figure}
  
  So, on each picture taken by the autonomous robot, we apply LSD to obtain segments. After that, the segments are 
classified into groups: \hcr{the}{} Verticals, \hcr{the }{}horizontals and \hcr{the }diagonals, up to a certain 
tolerance $\epsilon$. Then, in each group, we merge the closet segments up to a certain tolerance $\lambda$, in order to 
get a longer segments, we keep only the longest one from each group at the end. \hc{Each group of segments is used for a 
certain purpose for the navigation}.
  
The horizontal segments which are situated on the top of the image are ignored. We assume that theses segments represent 
a ceiling. We do not risk anything by ignoring theses segments because, even if the segments represent a frontier 
between the floor and far wall. We develop a strategy where the autonomous robot does not build a wall unless it is 
close to it.

    \subsection{3D map building}
    
   
    We remind that our goal is to build a rough 3D representation of the environment and to look for the target object. 
We represent the environment by defining the walls composing it. The detection of walls is based on LSD's segments. 
 Depending on the robot's position, the picture obtained may be in one of the general cases showed in Fig. 
\ref{fig:walls} below. 
    
    
    	\begin{figure}[H]
    	\begin{center}
    		\includegraphics[scale=0.65]{res/cases_seg.png}
    		\caption{Walls detection depending on the robot's position}
    		\label{fig:walls}
    	\end{center}
    \end{figure}
 
 \newpage
 The useful information that we can retrieve from each case is as follows:
 \begin{itemize}
 	\item Case A: 3 types of segments are detected, an horizontal one and 2 diagonals on 2 opposing walls.
 	\item Case B and C: 2 types of segments (horizontal and diagonal) sharing an endpoint are detected.
 	\item Case D: An horizontal segment is detected.
 	\item Case E: Any segment is detected.
 	
 \end{itemize}
 
 We mention that the vertical segments are used only to set the height of walls, they are not used in the exploration strategy because the autonomous robot is moving only along the X and Z axis.
 \subsection{Navigation}
     
 Navigation is the determination of an adequate and secure path for the autonomous robot, to move from a starting 
\hcr{spot}{point} to a target\hc{ed} one \cite{bonin-font_visual_2008}. On the basis of visual information, the 
autonomous robot is able to define its next decision. The decision may be to:
 \begin{itemize}
 	\item Stop if the target has been found or if the robot has explored the entire environment, without spotting the target.
 	\item Avoid obstacles.
 	\item Move back, move forward and rotate to continue exploring.
 \end{itemize}

\subsubsection{Obstacle avoidance}

In our previous work, when the autonomous robot decided to move forward, it did not check progressively while moving 
whether there is an obstacle or not. \hcr{But, it verified if there is an obstacle when it }{It checked for the 
occurrence of an obstacle after it } stopped moving. The drawback of such a decision is that the autonomous robot was 
not able to distinguish between its last position at the end of the movement and its position at the first moment when 
it met the obstacle. As a result, the autonomous robot thought that it travelled the entire distance.\\

In our new strategy, the robot stops as soon as it detects an obstacle. Its sensors may detect the presence 
of an obstacle at almost 10 centimeters. When the autonomous robot stops, the real distance travelled is returned. If 
there were no obstacles, the distance would be entirely covered.

\subsubsection{Strategy}

As there are a lot of \hcr{cases of movements according to}{differents decisions to be taken depending on} the visual 
information, we \hcr{try to } summarize these cases using an activity diagram \hcr{shown in the figure below}{(see Fig. 
\ref{fig:activityDiag})}. In this diagram, the green boxes represent a physical movement done by the autonomous robot, 
and the blue ones represent treatments.\\

We assume that the user has already mentioned the name of the object to find, and that the robot is situated in a closed 
environment. If we start with the initial point of the diagram, we can see that the autonomous robot begins by taking a 
picture and analyzes it. After that, it extracts the visual information and sends them to the computer, where the orders 
of movements are provided. 

From the collected information, if the target is found, the robot stops. Otherwise, if there are any segments detected, 
each of these segments is represented by a wall and added to the 3D map. At the end of the treatment, a movement is 
ordered to the autonomous robot (rotate, move back...etc). Then, it regains from the beginning until it stops.

\begin{figure}[H]
	\begin{center}
		\includegraphics[scale=0.445]{res/order_processing.png}
		\caption{Activity diagram of the exploration strategy}
		\label{fig:activityDiag}
	\end{center}
\end{figure}
\newpage
	

	\chapter{Testing}
	\label{chap:testing}
	
	\paragraph{}
	\section{Environment plan}
	We chose an environment which contained corridors, obstacles, textured walls, different level of light and different types of door. Such an environment is interesting to reveal the robustness of the strategy.
		
	\paragraph{}
	When launching the application, the target object is filled in. Then, the autonomous robot is going to build progressively its environment while searching the target object. The picture below represents the environment plan.
	\begin{figure}[H]
		\begin{center}
			\includegraphics[scale=0.55]{res/plan.png}
			\caption{The environment plan}
		\end{center}
	\end{figure}
	\paragraph{}
	We can see that the plan is composed of a principal corridor (A) with 5 simple doors and 3 double-doors. There are two double-doors in front of the simple ones which lead to 2 other corridors (B and C). Than, the autonomous robot is represented by a robot symbol and it is in front of a simple door in the principal corridor. The autonomous robot will try to find the target object which is represented by a red cross.

	\section{Image processing}
	\paragraph{}
	In this section, we present the environment structure detection in different spots. We precise that the vertical segments are green, the horizontal ones are red and the diagonal ones are blue.
		\begin{figure}[H]
		\begin{center}
			\includegraphics[scale=0.70]{res/img_proc.png}
			\caption{Segments detection testing}
		\end{center}
	\end{figure}
	\paragraph{}
	The segments shown in the figure above are not all considered. For example for those which are on the top of the image (On the ceiling), they are ignored by the algorithm later. Also, we remind that we keep only the longest segment from each category (Horizontal, vertical and diagonal).
	\paragraph{}
	The weak point of this algorithm is when the floor and the wall have almost the same color, we can see that in the figure C. However the algorithm encounters any difficulties while detecting segments on the brown wall. But, it fails at the detection of the frontier between the gray wall and the floor on the left.
	
	\paragraph{}
	Another weak point which may be noticed is when there is not enough lights in the room, as in the figure D. The wall on the left is nearly dark. Moreover both the wall and the floor are gray.
	
	\paragraph{}
	Beyond these weak points, the algorithm is able to extract the most important contours from the image, which remains sufficient for the decision. 
	\section{3D map building}
	\paragraph{}
	The picture below represents a 3D map representation of the environment at the end of the exploration. We precise that the environment has been built at a scale of 1/100 centimeters, hence the eventual impression that the walls are very offbeat.
	\paragraph{}
	At the beginning and as shown on the plan, the autonomous robot is in front of the door 5. As mentioned in the strategy that the autonomous robot goes always right while exploring the wall. Consequently, the autonomous robot explores almost all the environment before finding the target object.
	\begin{figure}[H]
		\begin{center}
			\includegraphics[scale=0.90]{res/3D_recons.png}
			\caption{3D map building of the environment}
		\end{center}
	\end{figure}
	
	\paragraph{}
	Now, we detail the 3D representation and we explain some details about it:
	\begin{figure}[H]
		\begin{center}
			\includegraphics[scale=0.90]{res/3D_recons_det.png}
			\caption{Discussion of the environment representation}
		\end{center}
	\end{figure}
	\begin{itemize}
		\item Case A: The autonomous robot is close to the opposite wall and it travels a short distance to explore a new part of the same wall. So, the recovery phenomenon can take place because a part of the explored wall at the first spot remains in the field of view of the autonomous robot at the second spot. As a result, we get two bunk walls.
		\item Case B: There is a gap between the double-door and the floor. As a result, the segment separating the wall and the double-door may sometimes not be detected. According to the strategy, the autonomous robot moves forward and meets an obstacle (The double-door). Consequently, it draws a wall with a small width and continues exploring the rest of the environment. The picture below shows that specific case.

			\begin{figure}[H]
			\begin{center}
				\includegraphics[scale=0.50]{res/no_segment.png}
				\caption{The grap between the double-door and the floor case}
			\end{center}
			\end{figure}
		We do not consider the vertical segment (In green)  and conclude that it is a wall, because the height does not guarantee the presence of a close wall. It may represent the height of an entire wall which is very far from the autonomous robot.
		
		
		\item Case C: At that spot, there is a big ventilation grid (fig. below). The autonomous robot detects one of the horizontal segment of the ventilation and considers it as a width of the wall in front. Unfortunately, the ventilation grid is lifted off the ground. Thus, The autonomous robot concludes that it's a border between the ground and a wall away from them, and places a far wall at C'.
	\begin{figure}[H]
	\begin{center}
		\includegraphics[scale=0.60]{res/grid_vent.png}
		\caption{Ventilation grid obstacle}
	\end{center}
\end{figure}
	
		\item Case D: We turn off the lights of the corridor 1 to see how the autonomous robot behaves. Naturally, the corridor is dark on the picture at this spot. As a result, any segment of the corridor 1 is detected. But, the autonomous robot is located in the principal corridor where the lights is on. And the floor of the principal corridor and the corridor 1 had different colors. Consequently, The limit between the two floors is detected as an horizontal segment and the autonomous robot believes that it is a wall.
		\begin{figure}[H]
			\begin{center}
				\includegraphics[scale=0.80]{res/limit_cor.jpg}
				\caption{Two floors with different colors case}
			\end{center}
		\end{figure}
	\item Case E: We keep the lights on in the corridor 2. We notice that the far wall of the corridor 2 is detected. Normally, the autonomous robot should move forward, because it detects too the diagonal segments which guarantee the presence of a depth. Nevertheless, we have omitted from our strategy the consideration of diagonals in decision. Because, a shadow was detected once as a diagonal in a case where there were any depth, and this misleads the autonomous robot.
	
	\item Cases F and G: Bearing in mind the margin of error in rotation and movement, the closed doors 2 and 4 are detected properly. However the doors 1, 4 and 5 may be missed and considered as a walls.
	\end{itemize}

	\paragraph{}
	After testing the strategy in a complex environment contrary to a simple rectangular room, we can judge that the strategy remains more generic and does not depend anymore of the environment structure, a specific objects substituting corners...etc. Also, we arrived at a precision at a scale of 1/100 centimeters. And the same exploration strategy runs at a scale of centimeters would have a smoother representation.
	\chapter*{Conclusion, Limitation and Future work}
	\addcontentsline{toc}{chapter}{Conclusion, Limitation and Future work}
	\paragraph{}
	During our internship, we focused the most on how to make the exploration strategy more generic. We can be somewhat satisfied with the work that has been done so far, comparing to our previous works and with respect to the internship's duration.

	\paragraph{}
	Nevertheless, we suggest some ideas of how our works can be improved:
	\begin{itemize}
		\item Multi-agent exploration: Instead of exploring the environment with only one autonomous robot, we can imagine having a group of autonomous robots doing so. In this case, the communication between the autonomous robots is to develop and the tasks to be shared.
		\item 3D object recognition: The current database contains only a 2D models. Works could be done in that field to manipulate a 3D models instead of 2D ones. This facilitates the recognition of the object at any views. Because now, the autonomous robot should be in front of the object to be able to recognize it.
		\item 2D/3D object detection with Deep Learning: If the database becomes large, the current object recognition may take a long time to analyze the picture. Because it tries to match the picture with each element of the database.
		\item Smooth environment representation: In our exploration strategy, a huge wall may be explored progressively. As a result, a parts of that wall are built side by side instead of building that wall as one piece. An improvement would be to merge that blocks of walls into one wall.
		
	\end{itemize}
\paragraph{}
To conclude, having an internship at LIP6 and being guided by an amazing supervisor provide us with unique opportunities for personal growth. I did not expect at my first day that I will learn as much knowledge at the end. Well, we come out of our internship further strengthened. 
	
	
\bibliographystyle{plain}
\bibliography{references}


\end{document}
